# Homework 15

## 1. Explain and compare following concepts, provide specific examples when doing comparison:
- Testing related:
  1. Unit Testing
  2. Functional Testing
  3. Integration Testing
  4. Regression Testing
  5. Smoke Testing
  6. Performance Testing
  7. A/B Testing
  8. User Acceptance Testing
- Environment related:
  1. Development
  2. QA (Quality Assurance)
  3. Pre-prod/Staging
  4. Production


### **Testing Types:** 

| Concept              | Definition                                                                 | Example                                                                                     | Key Focus                              |
|----------------------|----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|----------------------------------------|
| **1. Unit Testing**   | Tests individual components (e.g., functions, classes) in isolation.      | Testing a `calculateTotal()` function to ensure it returns the correct sum of cart items.   | Code correctness at the smallest level.|
| **2. Functional Testing** | Validates end-to-end functionality against requirements.               | Testing a login feature to ensure users can authenticate with valid credentials.           | User-facing workflows.                 |
| **3. Integration Testing** | Tests interactions between components/modules.                          | Testing if a payment gateway correctly integrates with an e-commerce checkout system.      | Component collaboration.               |
| **4. Regression Testing** | Ensures new changes don’t break existing functionality.                 | Re-running tests for a search feature after updating the UI to confirm it still works.      | Stability after changes.               |
| **5. Smoke Testing**  | Quick check to confirm basic system functionality.                        | After deployment, verifying the app launches and the home page loads without errors.        | "Sanity check" for critical paths.     |
| **6. Performance Testing** | Measures system responsiveness under load (e.g., speed, scalability).  | Simulating 10,000 concurrent users to test if a video streaming service handles the load.   | Scalability and resource usage.        |
| **7. A/B Testing**    | Compares two versions of a feature to determine which performs better.    | Testing a green "Buy Now" button (Version A) vs. a red one (Version B) to see which drives more sales. | User behavior analysis.                |
| **8. User Acceptance Testing (UAT)** | Final validation by end-users to confirm the system meets their needs. | Letting a client test a new inventory management system before signing off on delivery.     | Alignment with business requirements.  |

---

### **Environment Types:** 

| Concept                  | Purpose                                                                 | Example                                                                                     | Key Characteristics                   |
|--------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|----------------------------------------|
| **1. Development**        | Used by developers to write and debug code.                            | A local machine running a Node.js server and a test database for building a feature.        | Unstable, frequent changes, debug tools. |
| **2. QA (Quality Assurance)** | Dedicated environment for rigorous testing (e.g., functional, regression). | A server mimicking production where testers validate bug fixes and new features.            | Mirrors production but isolated.       |
| **3. Pre-prod/Staging**   | Final testing environment before deployment to production.             | A replica of production used to test deployment scripts and final user acceptance.          | Nearly identical to production.        |
| **4. Production**         | Live environment where end-users interact with the system.             | A cloud-hosted SaaS application serving real customers with load balancers and monitoring.  | High stability, monitoring, backups.   |

---

### **Testing Types**
- **Unit vs. Functional Testing**:  
  - Unit tests focus on isolated code logic (e.g., testing a sorting algorithm).  
  - Functional tests validate user journeys (e.g., placing an order).  

- **Integration vs. Smoke Testing**:  
  - Integration tests ensure modules work together (e.g., API and database).  
  - Smoke tests check basic system health (e.g., "Does the app start?").  

- **Regression vs. UAT**:  
  - Regression tests prevent breaking existing features.  
  - UAT ensures the system meets business/user needs.  

### **Environments**
- **Development vs. QA**:  
  - Development is for coding and debugging (unstable).  
  - QA is for systematic testing (stable but isolated).  

- **Staging vs. Production**:  
  - Staging mirrors production but isn’t public.  
  - Production is live and requires high availability.  


## 2. What is the lifecircle of Junit?
- **JUnit 4 Lifecycle**  
  1. **Annotations**:  
    - `@BeforeClass` (static method): Runs **once** before all test methods.  
    - `@Before`: Runs **before each** test method.  
    - `@Test`: Marks a test method.  
    - `@After`: Runs **after each** test method.  
    - `@AfterClass` (static method): Runs **once** after all test methods.  

  2. **Execution Flow**:  
    - `@BeforeClass` → (`@Before` → `@Test` → `@After`) for each test → `@AfterClass`.  
    - **Test Isolation**: A new instance of the test class is created for each test method.  



- **JUnit 5 Lifecycle**  
  1. **Annotations**:  
    - `@BeforeAll`: Runs **once** before all test methods (static by default).  
    - `@BeforeEach`: Runs **before each** test method.  
    - `@Test`: Marks a test method.  
    - `@AfterEach`: Runs **after each** test method.  
    - `@AfterAll`: Runs **once** after all test methods (static by default).  

  2. **Execution Flow**:  
    - Default (`PER_METHOD`):  
      `@BeforeAll` → (`@BeforeEach` → `@Test` → `@AfterEach`) for each test → `@AfterAll`.  
    - Class-Level (`PER_CLASS`):  
      Add `@TestInstance(Lifecycle.PER_CLASS)` to reuse the same test instance across methods.  
      - `@BeforeAll`/`@AfterAll` can be non-static.  

  3. **Key Differences from JUnit 4**:  
    - More flexible lifecycle configuration (e.g., `PER_CLASS` mode).  
    - No forced use of `static` for `@BeforeAll`/`@AfterAll` in `PER_CLASS` mode.  
    - Built-in support for nested tests and extensions.  



- **Why the Lifecycle Matters**  
  - **Isolation**: Ensures tests run independently (e.g., via `@BeforeEach`).  
  - **Efficiency**: `@BeforeAll`/`@AfterAll` reduce setup costs for shared resources.  
  - **Clarity**: Clear separation of setup, execution, and teardown phases.  



## 3. Explain parameterized testing?
**Parameterized testing** is a data-driven testing approach that allows a single test method to be executed multiple times with varying input values. Instead of writing separate tests for each input combination, you define a **generic test logic** once and supply different parameters dynamically. This technique enhances test coverage and efficiency by systematically validating diverse scenarios.



- **Key Components**  
  1. **Test Logic**  
    - A reusable method that defines the validation steps (e.g., verifying outputs against expected results).  

  2. **Parameter Sources**  
    - Input data is provided externally through:  
      - **Hardcoded values**: Predefined lists or arrays.  
      - **Files**: CSV, JSON, or Excel files.  
      - **Databases/APIs**: Dynamic data fetched from external systems.  

  3. **Framework Support**  
    - Testing frameworks (e.g., JUnit, TestNG, pytest) provide annotations or tools to manage parameterized execution.  


- **Why Use Parameterized Testing?**  
  1. **Efficiency**  
    - Eliminates redundant code by reusing test logic for multiple inputs.  
  2. **Comprehensive Coverage**  
    - Tests edge cases, boundary values, and diverse scenarios systematically.  
  3. **Maintainability**  
    - Centralized data management simplifies updates and reduces errors.  


- **Common Use Cases**  
  - **Mathematical Functions**: Validate outputs for various input combinations (e.g., positive/negative numbers).  
  - **API Validation**: Test endpoints with different query parameters or headers.  
  - **Business Rules**: Verify behavior under varying conditions (e.g., user roles, discounts, thresholds).  


- **How It Works**  
  1. **Define the Test Logic**: Write a single test method to handle validation.  
  2. **Supply Parameters**: Use a data source (e.g., CSV file) to inject inputs.  
  3. **Execute Iteratively**: The framework runs the test method for each parameter set, treating each iteration as an independent test.  


- **Benefits**  
  - **Reduced Duplication**: Avoid writing repetitive test methods.  
  - **Scalability**: Easily add new test cases by updating data sources.  
  - **Clarity**: Separates test logic from test data for better readability.  


- **Best Practices**  
  1. **Use Descriptive Names**: Clearly label test cases to reflect input scenarios.  
  2. **Isolate Failures**: Ensure one failing input doesn’t block others.  
  3. **Keep Data Manageable**: Use simple formats (e.g., CSV) for readability.  

Parameterized testing is ideal for scenarios requiring systematic validation of multiple input combinations while maintaining clean and maintainable test suites. 


## 4. Explain Mockito and PowerMock . 
Mockito is a mocking framework for Java that simplifies the creation and management of mock objects in unit tests. It focuses on behavior verification and stubbing to isolate code under test from external dependencies.

- **Core Features**  
  - **Mock Creation**: Easily create mock objects for interfaces or classes.  
  - **Stubbing**: Define mock behavior (e.g., return values, exceptions).  
  - **Verification**: Check if specific interactions occurred (e.g., `verify(mock).methodCall()`).  
  - **Spies**: Partial mocking of real objects (e.g., `spy(RealClass.class)`).  

- **Limitations**  
  - Cannot mock **static methods**, **final classes**, or **private methods**.  
  - Requires well-designed code with dependency injection for optimal use.  

- **Typical Use Cases**  
  - Mocking service layers (e.g., DAOs, APIs) to test business logic.  
  - Verifying interactions with collaborators (e.g., "Was this method called?").  




**PowerMock** is an extension of Mockito (and other frameworks) that overcomes mocking limitations by using bytecode manipulation. It enables mocking of traditionally "unmockable" code.

- **Core Features**  
  - **Static Method Mocking**: Mock static methods (e.g., `PowerMockito.mockStatic(Class.class)`).  
  - **Constructor Mocking**: Suppress or stub object creation (e.g., `whenNew(Class.class)`).  
  - **Final Class/Private Method Mocking**: Bypass restrictions on final classes and private methods.  
  - **Integration**: Works with Mockito and JUnit/TestNG.  

- **Limitations**  
  - Increases test complexity and execution time.  
  - May encourage poor design (e.g., overusing static methods).  

- **Typical Use Cases**  
  - Testing legacy code with static utilities (e.g., `Logger`, `DateTimeUtils`).  
  - Mocking third-party libraries with restrictive APIs.  



- **Key Comparison**  
  | Aspect                | Mockito                                  | PowerMock                                |  
  |-----------------------|------------------------------------------|------------------------------------------|  
  | **Scope**             | Standard mocking (non-static methods).  | Advanced mocking (static/final/private). |  
  | **Design Philosophy** | Encourages clean, testable code.         | Compensates for untestable code.         |  
  | **Performance**       | Lightweight and fast.                    | Slower due to bytecode manipulation.     |  
  | **Use Case**          | Modern, well-architected systems.        | Legacy systems or restrictive codebases. |  



- **When to Use Which?**  
  - **Prefer Mockito** for new code with dependency injection and minimal static usage.  
  - **Use PowerMock** only when unavoidable (e.g., testing legacy code with static calls).  


## 5. Compare @Mock and @InjectMock
- **Purpose**  
  | Annotation      | Role                                                                 |  
  |-----------------|----------------------------------------------------------------------|  
  | **`@Mock`**     | Creates a mock object of a dependency (e.g., service, repository).  |  
  | **`@InjectMocks`** | Instantiates the class under test and injects `@Mock` dependencies. |  



- **Key Differences**  
  | Aspect                | `@Mock`                                  | `@InjectMocks`                          |  
  |-----------------------|------------------------------------------|------------------------------------------|  
  | **Focus**             | Mocks dependencies of the class under test. | Creates the class under test with injected mocks. |  
  | **Initialization**    | Requires explicit initialization (e.g., `MockitoAnnotations.openMocks()`). | Depends on `@Mock` fields to inject. |  
  | **Use Case**          | Isolate external collaborators (e.g., APIs, databases). | Test the behavior of the class under test with mocked dependencies. |  



- **How They Work Together**  
  1. **Define Dependencies**: Use `@Mock` to create mock objects for external services.  
  2. **Inject Dependencies**: Use `@InjectMocks` to instantiate the class under test and auto-inject the mocks.  
  3. **Initialize**: Ensure annotations are processed via `MockitoJUnitRunner` or `MockitoAnnotations.openMocks()`.  



- **When to Use**  
  - **`@Mock`**:  
    - To replace slow, unstable, or complex dependencies.  
    - To verify interactions (e.g., "Was this method called?").  
  - **`@InjectMocks`**:  
    - To test the class under test in isolation with mocked collaborators.  
    - To avoid manual dependency setup (e.g., constructor calls).  



- **Best Practices**  
  - **Combine Both**: Use `@Mock` for dependencies and `@InjectMocks` for the test target.  
  - **Avoid Overuse**: Mock only necessary dependencies to keep tests meaningful.  
  - **Initialize Properly**: Always trigger annotation processing to prevent `NullPointerException`.  


## 6. Explain stubbing.
  
Stubbing is a testing technique where predefined responses are provided for method calls to dependencies (e.g., APIs, databases, services). It replaces real implementations with simplified, controlled behavior to isolate the code under test.



- **Purpose of Stubbing**  
  - **Isolate Code**: Test a component without relying on external systems.  
  - **Control Behavior**: Simulate specific scenarios (e.g., success, errors, edge cases).  
  - **Speed Up Tests**: Avoid slow or unreliable operations (e.g., network calls).  



- **Stubbing vs. Mocking**  
  | Aspect          | Stubbing                                  | Mocking                                  |  
  |-----------------|-------------------------------------------|------------------------------------------|  
  | **Focus**       | Defines *what* a dependency returns.      | Verifies *how* a dependency is used.     |  
  | **Use Case**    | "If X is called, return Y."               | "Ensure X was called with parameters Z." |  



- **Common Scenarios**  
  - **API Calls**: Return canned responses instead of making real HTTP requests.  
  - **Database Queries**: Simulate query results without connecting to a database.  
  - **Error Handling**: Force exceptions to test failure paths (e.g., network timeout).  



- **Advantages**  
  - **Predictability**: Tests behave consistently across environments.  
  - **Speed**: Eliminates delays from external dependencies.  
  - **Simplicity**: Reduces test setup complexity.  



- **Best Practices**  
  1. **Target Critical Paths**: Stub only dependencies directly impacting the test.  
  2. **Keep It Simple**: Avoid over-stubbing to prevent tests from diverging from real behavior.  
  3. **Update Regularly**: Ensure stubs reflect current dependency behavior. 



## 7. what is Mockito ArgumentMatchers.
ArgumentMatchers are Mockito utilities that provide flexible parameter matching during test stubbing or verification. They allow you to define or validate method calls without specifying exact argument values, making tests more adaptable to dynamic inputs.



- **Key Purposes**  
  1. **Parameter Flexibility**  
    - Match arguments based on type, conditions, or custom logic (e.g., "any string" or "non-null values").  
  2. **Simplified Testing**  
    - Avoid hardcoding specific values, reducing test brittleness.  



- **Common ArgumentMatchers**  
  | Matcher                   | Usage                                                                 |  
  |---------------------------|-----------------------------------------------------------------------|  
  | `any()`                   | Matches any object (including `null`).                               |  
  | `any(Class<T>)`           | Matches any object of a specific type (e.g., `any(String.class)`).   |  
  | `eq(value)`               | Matches an argument equal to the specified value.                    |  
  | `isNull()` / `isNotNull()`| Matches `null` or non-`null` arguments.                              |  
  | `argThat(condition)`      | Matches arguments using custom logic (e.g., regex, range checks).    |  



- **Use Cases**  
  1. **Stubbing Method Behavior**  
    - Define responses for methods called with **any valid argument** (e.g., `when(userService.save(any())).thenReturn(true)`).  
  2. **Verifying Interactions**  
    - Check if a method was called with specific argument types or conditions (e.g., `verify(logger).error(any(String.class))`).  



- **Caveats**  
  1. **Mixing Matchers**  
    - If one argument uses a matcher, **all arguments must use matchers** (except `eq()`).  
  2. **Type Safety**  
    - Prefer `any(Class<T>)` over raw `any()` to avoid type-related errors.  
  3. **Overuse Risks**  
    - Excessive use can mask bugs by ignoring argument specifics.  



- **Matchers vs. Exact Values**  
  | Approach                | Pros                                  | Cons                          |  
  |-------------------------|---------------------------------------|-------------------------------|  
  | **Exact Values**        | Clear and precise.                    | Brittle (breaks on minor changes). |  
  | **ArgumentMatchers**    | Flexible and resilient to changes.    | May reduce test specificity.  |  


## 8. Compare @spy and @Mock?
- **Core Differences**  
  | Aspect                | `@Mock`                                  | `@Spy`                                   |  
  |-----------------------|------------------------------------------|------------------------------------------|  
  | **Object Type**       | Creates a **full mock** of a class/interface. | Wraps a **real instance** (partial mock). |  
  | **Default Behavior**  | All methods return defaults (e.g., `null`, `0`). | Calls **real methods** unless stubbed. |  
  | **Use Case**          | Isolate dependencies completely.         | Test real logic while overriding specific methods. |  
  | **Initialization**    | Requires no real object.                 | Requires a real instance (e.g., `@Spy(new MyClass())`). |  
  | **Overhead**          | Lightweight, no real logic executed.     | Heavier, as real methods may have side effects. |  



- **When to Use**  
  - **`@Mock`**:  
    - To **fully isolate** a dependency (e.g., external services, databases).  
    - When you need to define **all interactions explicitly**.  
  - **`@Spy`**:  
    - To **partially mock** an object (e.g., override one method but keep others real).  
    - When testing legacy code where refactoring to dependency injection is impractical.  



- **Example Scenarios**  
  1. **`@Mock`**:  
    - Mocking a `PaymentGateway` to avoid real API calls during testing.  
  2. **`@Spy`**:  
    - Spying on a `DataProcessor` to stub a slow `validate()` method while using the real `process()` logic.  



- **Best Practices**  
  1. **Prefer `@Mock`** for most cases to ensure clean isolation.  
  2. **Use `@Spy` sparingly**: Real method calls can introduce unintended side effects.  
  3. **Avoid `@Spy` on Spring Beans**: Prefer `@MockBean` in Spring tests for better control.

## 9. Explain Assertion . 
An assertion is a runtime check that verifies a condition expected to be true during code execution. If the condition fails, an assertion error is thrown, indicating a logical flaw in the program. Assertions are primarily used for debugging and testing to enforce correctness.



- **Key Purposes**  
  1. **Validate Assumptions**: Ensure critical assumptions in the code hold true (e.g., "this variable is never null").  
  2. **Catch Bugs Early**: Identify logic errors during development and testing.  
  3. **Document Intent**: Make code behavior explicit (e.g., "this method requires a positive input").  



- **Types of Assertions**  
  1. **Language-Level Assertions**  
    - Built into programming languages (e.g., Java’s `assert` keyword, Python’s `assert` statement).  
  2. **Testing Framework Assertions**  
    - Provided by testing tools (e.g., JUnit’s `assertEquals()`, pytest’s `assert`).  



- **When to Use Assertions**  
  - **Preconditions**: Validate inputs or states before executing logic.  
  - **Postconditions**: Verify outputs or side effects after logic execution.  
  - **Invariants**: Ensure conditions remain true during execution (e.g., loop invariants).  



- **Assertions vs. Exceptions**  
  | Aspect          | Assertions                                  | Exceptions                                |  
  |-----------------|---------------------------------------------|-------------------------------------------|  
  | **Purpose**     | Catch **logic errors** (bugs).             | Handle **expected runtime errors** (e.g., invalid input). |  
  | **Usage**       | Enabled in debug/test environments.        | Always active in production.              |  
  | **Recovery**    | Not intended for recovery; indicates fatal flaws. | Can be caught and handled gracefully.     |  



- **Best Practices**  
  1. **Avoid Side Effects**: Assertions should not modify program state.  
  2. **Use Sparingly in Production**: Disable assertions in production to avoid performance overhead (language-dependent).  
  3. **Complement with Tests**: Use testing frameworks for comprehensive validation beyond simple assertions.  



- **Limitations**  
  - **Not for User Input Validation**: Use exceptions for invalid user inputs.  
  - **Silent Failures**: Disabled assertions in production may hide undetected bugs.























