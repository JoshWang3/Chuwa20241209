### Explain following concepts, and how they coordinate with each other:

---

#### 1. **Topic**

A **topic** in Kafka is a logical name or channel to which messages are published. It acts like a category for organizing messages. All producers send data to a topic, and consumers read from it.

**Example**: A `user-signup` topic may carry all messages related to new user registrations.

---

#### 2. **Partition**

A **partition** is a horizontal split of a topic. Each topic can have multiple partitions, which allows data to be **distributed and processed in parallel**. Messages are assigned to partitions based on keys or round-robin logic.

**Purpose**: Enables scalability and parallel consumption.

---

#### 3. **Broker**

A **broker** is a single Kafka server that stores data and serves client requests. In a Kafka cluster, multiple brokers share the load and host different partitions of topics.

**Example**: A cluster with 3 brokers may host different partitions of the same topic across all brokers.

---

#### 4. **Consumer Group**

A **consumer group** is a set of consumer instances that **work together** to read messages from a topic. Kafka ensures that each partition is read by only **one consumer in a group** at any time, enabling parallelism and load balancing.

**Example**: In a group of 3 consumers and a topic with 3 partitions, each consumer reads from one partition.

---

#### 5. **Producer**

A **producer** is any application or component that sends data to Kafka topics. It chooses which topic to send the data to and optionally which partition.

**Example**: A web app sends user activity logs to the `user-activity` topic using a Kafka producer.

---

#### 6. **Offset**

An **offset** is a unique, sequential number assigned to each message in a partition. It tracks the message’s position and is used by consumers to **know where to resume reading**.

**Example**: If a consumer’s last read offset is 12, the next message it reads will be at offset 13.

---

#### 7. **Zookeeper**

**Zookeeper** is used by Kafka to manage cluster coordination tasks like:

- Tracking active brokers
- Electing partition leaders
- Managing consumer group metadata

_Note_: Kafka is gradually removing the dependency on Zookeeper in newer versions (KRaft mode).

---

### How They Coordinate:

- **Producers** send data to **topics**, which are split into **partitions**.
- **Brokers** host partitions and handle incoming data from producers and outgoing data to consumers.
- **Consumers** in a **consumer group** read from different partitions for scalability.
- **Offsets** track how far each consumer has read.
- **Zookeeper** ensures coordination of brokers and metadata across the cluster.

### Answer following questions:

### 1. Given N (number of partitions) and M (number of consumers,) what will happen when N>=M and N<M respectively?

---

#### Case 1: **N ≥ M** (Partitions are more than or equal to Consumers)

- Each **consumer** in the group is assigned **at least one partition**.
- Some consumers may handle multiple partitions.
- All consumers are active and processing data.
- This is an **efficient and balanced setup**, maximizing parallel processing.

**Example**:  
N = 4 partitions, M = 2 consumers  
→ Each consumer gets 2 partitions

---

#### Case 2: **N < M** (Partitions are fewer than Consumers)

- Not all consumers will be used — **some remain idle**.
- Since each partition can be assigned to **only one consumer** in a group, the number of **active consumers is limited to N**.
- This setup **wastes resources** and doesn’t provide any performance benefit from the extra consumers.

**Example**:  
N = 2 partitions, M = 5 consumers  
→ Only 2 consumers will be assigned partitions; the other 3 stay idle

---

### 2. Explain how brokers work with topics?

---

### Producer → Broker Flow:

1. A **producer** sends a message to a topic.
2. It connects to **any broker**, which acts as a **bootstrap server**.
3. Kafka determines the correct **partition** based on:
   - A message key (for consistent routing), or
   - Round-robin (if no key is given)
4. The message is sent to the **leader broker** of that partition.
5. The leader broker **writes the message to disk**.
6. **Follower brokers** for that partition **replicate the data** to ensure redundancy and fault tolerance.

---

### Consumer ← Broker Flow:

1. A **consumer** connects to a broker and joins a **consumer group**.
2. Kafka assigns **partitions** to the consumer based on the group.
3. The consumer pulls messages from the **leader broker** of its assigned partitions.
4. It starts reading from a specific **offset** and updates the offset as it consumes new messages.

---

### 3. Are messages pushed to consumers or consumers pull messages from topics?

---

In **Kafka**, messages are **not pushed** to consumers.

Instead, **consumers pull messages** from topics at their own pace. This design gives consumers more control over:

- **How fast** they consume messages
- **Where to resume** (based on offsets)
- **Load management** without being overwhelmed

---

### Advantages of Pull-Based Model:

- Consumers can **control the flow** and implement backpressure.
- Easy to implement **offset tracking and retries**.
- Supports **scaling** with consumer groups.

---

### 4. How to avoid duplicate consumption of messages?

---

To avoid duplicate message consumption in Kafka, you can implement **message deduplication logic** at the consumer side.

---

### Common Strategy: Message ID Tracking

1. **Assign a Unique ID**

   - The producer attaches a **unique message ID** (e.g., UUID) to each message when publishing.

2. **Track Processed IDs**

   - The consumer stores each successfully processed message ID in a **Message ID Tracking Table** (typically in a database).

3. **Check Before Processing**
   - Before processing a message, the consumer checks if the ID already exists in the tracking table.
   - If it exists → **skip processing** (already handled).
   - If not → **process and store the ID**.

---

### 5. What will happen if some consumers are down in a consumer group? Will data loss occur? Why?

- If some consumers within a consumer group go down, Kafka's design—including durable storage, offset management, and consumer group rebalancing—ensures that no data is lost. The remaining consumers take over the processing of the affected partitions, and once the downed consumers recover, they can rejoin the g---

If some consumers in a Kafka consumer group **go down**, **data will NOT be lost**.

Kafka is designed with features that ensure **reliability and fault tolerance**, even when consumers fail.

---

### What Happens:

1. **Rebalancing**

   - Kafka automatically triggers a **rebalance**.
   - The remaining consumers **take over** the partitions assigned to the failed ones.

2. **Offset Management**

   - Kafka tracks the **offsets** of each consumer.
   - When a consumer restarts, it can **resume from the last committed offset**, ensuring no messages are lost or duplicated.

3. **Durable Storage**
   - Messages are stored durably on Kafka brokers until they are consumed or expire (based on retention settings).

---

### Why No Data Loss?

- Kafka **does not delete messages immediately** after consumption.
- Offsets are managed separately, allowing consumers to resume where they left off.
- **Message replication** across brokers ensures availability even if a broker or consumer fails.

---roup and resume processing from the correct offsets.

### 6. What will happen if an entire consumer group is down? Will data loss occur? Why?

If an **entire consumer group** goes down, **no data will be lost**.

Kafka ensures durability and reliability through its **persistent storage** and **offset tracking** mechanisms.

### 7. Explain consumer lag and how to resolve it?

---

### What Is Consumer Lag?

**Consumer lag** is the difference between the **latest message offset** in a Kafka partition and the **last committed offset** by a consumer. It measures how far **behind** a consumer is in processing incoming messages.

**Formula:**

```
Consumer Lag = Latest Offset - Committed Offset
```

---

### How to Resolve Consumer Lag:

1. **Scale Out Consumers**

   - Add more consumers to the consumer group to parallelize message processing.

2. **Optimize Consumer Logic**

   - Improve the performance of your message handling code (e.g., avoid heavy computation or blocking calls).

3. **Increase Topic Partitions**

   - More partitions allow better distribution of load across consumers.

4. **Tune Consumer Configs**

   - Adjust parameters like `fetch.min.bytes`, `max.poll.records`, and `max.poll.interval.ms`.

5. **Monitor and Alert**
   - Use tools like Prometheus, Grafana, or Kafka’s built-in JMX metrics to monitor lag in real time.

---

### 8. Explain how Kafka tracks message delivery?

---

Kafka tracks message delivery using a concept called **offsets**.

---

### What is an Offset?

- An **offset** is a unique, sequential number assigned to each message within a **partition** of a Kafka topic.
- It acts like a pointer that keeps track of **where the consumer is** in the message stream.

---

### How Kafka Uses Offsets:

1. **Producer Side**

   - When a message is published to a topic, it is stored in a specific partition and assigned an offset.
   - Kafka does **not track delivery status**, but focuses on reliable storage.

2. **Consumer Side**

   - Consumers **read messages based on offsets**.
   - After processing a message, the consumer **commits the offset**, marking it as "read."

3. **Offset Commit**
   - Can be done **automatically** or **manually**.
   - Stored in Kafka's internal `__consumer_offsets` topic.

---

### 9. Compare Kafka vs RabbitMQ, compare messageing frameworks vs MySql (Why Kafka)?

### 9. Compare Kafka vs RabbitMQ, and Messaging Frameworks vs MySQL (Why Kafka?)

---

### Kafka vs RabbitMQ

| Feature              | Kafka                                       | RabbitMQ                                                  |
| -------------------- | ------------------------------------------- | --------------------------------------------------------- |
| **Message Model**    | Log-based, pull-based                       | Queue-based, push-based                                   |
| **Throughput**       | High, optimized for large volumes           | Lower throughput, better for small messages               |
| **Message Ordering** | Guaranteed within a partition               | Not guaranteed across queues                              |
| **Persistence**      | Stores messages on disk by default          | Messages are deleted once consumed (unless durable queue) |
| **Consumer Model**   | Consumers pull messages (at their own pace) | Messages pushed to consumers                              |
| **Use Case**         | Event streaming, real-time analytics        | Traditional messaging (e.g., tasks, RPC)                  |

**Summary**:

- Use **Kafka** when you need high throughput, durability, and real-time data processing.
- Use **RabbitMQ** for traditional queuing scenarios with complex routing and lower volume.

---

### Messaging Frameworks vs MySQL (Why Kafka?)

| Aspect          | Kafka/Messaging Frameworks           | MySQL (or Traditional DB)        |
| --------------- | ------------------------------------ | -------------------------------- |
| **Purpose**     | Streaming & messaging                | Data storage & retrieval         |
| **Data Flow**   | Real-time, event-driven              | Request/response-based           |
| **Scalability** | Highly scalable (distributed design) | Limited to DB scale              |
| **Decoupling**  | Decouples producers and consumers    | Tight coupling                   |
| **Use Case**    | Event pipelines, async processing    | Structured data storage, queries |

**Why Kafka?**

- Designed for **distributed, fault-tolerant, real-time event streaming**.
- Provides **high throughput**, **data durability**, and **consumer independence**.
- Ideal for modern microservice architectures where services communicate via **event-driven messaging** rather than database polling.

---

### 10. On top of https://github.com/CTYue/Spring-Producer-Consumer Write your consumer application with Spring Kafka dependency, set up 3 consumers in a single consumer group.

### Prove message consumption with screenshots.Increase number of consumers in a single consumer group, observe what happens, explain your observation.Create multiple consumer groups using Spring Kafka, set up different numbers of consumers within each group, observe consumer offset,

### Prove that each consumer group is consuming messages on topics as expected, take screenshots of offset records,Demo different message delivery guarantees in Kafka, with necessary code or configuration changes.

### Note: There's already a docker compose file in above git repo, you can start the 3 brokers (with zookeep) using docker-compose up command.
