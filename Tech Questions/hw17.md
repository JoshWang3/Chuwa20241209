# Homework 17
## Kafka

Explain following concepts, and how they coordinate with each other:

- Topic
- Partition
- Broker
- Consumer group
- Producer
- Offset
- Zookeeper

A **topic** is a **category or feed name** to which data is written by producers and read by consumers. It's a **logical channel** where messages with a common purpose are published. Topics are divided into **partitions** for better performance and scalability.

A **partition** is a subdivision of a topic. Each partition **holds** a portion of the **topic's messages** in a specific order. Kafka distributes partitions across multiple brokers to parallelize message handling and provide fault tolerance. If a topic has three partitions, messages can be distributed across Partition 0, Partition 1, and Partition 2.

A **broker** is a **Kafka server** responsible for storing and managing messages for partitions. Brokers handle requests from producers to write data and from consumers to read data. Kafka typically runs as a cluster of multiple brokers. If a topic has three partitions, each partition might be handled by different brokers.

A **consumer group** is a collection of consumers working together to **read messages** from partitions of a topic. Kafka ensures that each partition is read by only one consumer within a group at a time, enabling load balancing. If a topic has 4 partitions and a consumer group has 2 consumers, each consumer reads messages from 2 partitions.

A **producer** publishes messages to a Kafka topic. It decides which partition to **write messages** to based on a partitioning strategy.

An **offset** is a unique **identifier** for each message within a partition. Consumers use offsets to track which messages they have read and processed. Partition 1 might have messages at offset positions 0, 1, 2, and so on.

**Zookeeper** is a distributed coordination service used by Kafka to manage cluster metadata, track broker statuses, and perform leader election for partitions. Kafkaâ€™s newer versions are moving towards replacing Zookeeper with an internal quorum-based controller.

**How They Coordinate**  

1. **Producer to Topic**: A producer sends messages to a specific topic, which Kafka assigns to partitions based on a partitioning strategy.

2. **Partitioning & Brokers**: Partitions are distributed across brokers, which store and manage the data.

3. **Zookeeper Coordination**: Zookeeper keeps track of the brokers, assigns partition leaders, and ensures fault tolerance through leader election.

4. **Consumers & Consumer Groups**: Consumers in the same group divide the partitions among themselves to read data efficiently without overlaps.

5. **Offset Tracking**: Kafka tracks message offsets so consumers know where to resume reading after a restart or failure. 


### Answer following questions:
### 1. Given N (number of partitions) and M (number of consumers,) what will happen when N>=M and N<M respectively?
**Case 1: N â‰¥ M (Number of partitions is greater than or equal to the number of consumers)**  
Each consumer will read from one or more partitions. Kafka ensures that all partitions are assigned to consumers without overlap. Some consumers may handle multiple partitions to balance the workload.
**Load balancing** across consumers can be uneven if the number of partitions doesn't divide evenly among consumers.
**Case 2: N < M (Number of partitions is less than the number of consumers)**  
Some consumers will be idle because Kafka does not allow multiple consumers within the same group to read from the same partition. Only **N** consumers will actively read, while the remaining **M - N** consumers will be idle. 
**Wasted resources** since some consumers do no work. 

### 2. Explain how brokers work with topics?
1. When a topic is **created**, it is divided into **partitions** for parallelism and scalability. Messages within a partition are stored in the order they are received, indexed by **offsets**. Kafka assigns each partition to a **broker** in the cluster. Each broker **stores** the data for the partitions it owns. 
2. Kafka distributes partitions across brokers to balance the **load evenly**. This distribution allows Kafka to handle large volumes of data and ensures **fault tolerance**.
3. For each partition, one broker acts as the **leader**, and others may act as **followers**. Brokers replicate partition data based on the topic's **replication factor**. This replication ensures that even if a broker fails, data isn't lost.
4. Producers and consumers communicate only with the **leader broker** for a partition. If a leader broker goes down, Kafka promotes one of the followers as the new leader. Zookeeper manages leader elections and broker tracking.
5. **Producers** send messages to brokers responsible for the target partition determined by a partitioning strategy. **Consumers** request messages from brokers that are leaders of the partitions they are assigned.

### 3. Are messages pushed to consumers or consumers pull messages from topics?
In Kafka, **consumers pull messages from topics** rather than the system pushing messages to them. **Why Pull-Based Model?**
1. **Consumer Control:** Consumers control the rate at which they retrieve messages. They can request messages based on their processing capability. If a consumer is slow, it simply pulls fewer messages ensuring they are not overwhelmed by too much data (backpressure handling).
2. **Offset Tracking:** Kafka relies on consumers to commit offsets after successfully processing messages. With a pull model, consumers manage this process explicitly.
3. **Efficiency in Batching:** Kafka allows consumers to pull messages in batches rather than one at a time. This improves efficiency and reduces latency.

### 4. How to avoid duplicate consumption of messages?
**1. Use Consumer Offset Management Properly**
- Because if a consumer crashes before committing, Kafka assumes messages were not processed and reassigns them, leading to duplicates.
- **Enable Auto Commit Carefully:** Kafka can auto-commit offsets (`enable.auto.commit=true`), but this can lead to duplicate processing if a failure occurs after consuming but before processing.
- **Manual Offset Management:** Manually commit offsets (`enable.auto.commit=false`) only after successful message processing. `consumer.commitSync();`

**2. Leverage Idempotent Producers**
Producers can set `enable.idempotence=true` to ensure that retries for message sending produce no duplicates. Kafka tracks unique message IDs (`Producer ID + Sequence Number`) to avoid writing duplicate messages.
**3. Deduplicate Messages at the Consumer Level**
Consumers can maintain a **message processing log** or **hash set** to track message IDs (based on custom keys or Kafka offsets). If a database operation has a unique constraint, attempt the insert and handle duplicate key exceptions gracefully.

**4. Use Kafka Streams API for Exactly-Once Guarantees**
Kafka Streams handles offsets and transactions internally, ensuring exactly-once processing without requiring manual management.

### 5. What will happen if some consumers are down in a consumer group? Will data loss occur? Why?
If some consumers in a Kafka **consumer group** go down, **data loss will not occur**, thanks to Kafka's design and fault-tolerant mechanisms. 

**Partition Rebalancing**: When a consumer goes down, Kafka detects the failure via **heartbeat timeouts** and triggers a **rebalance** in the consumer group. The partitions assigned to the failed consumer are reassigned to the remaining active consumers. The remaining consumers continue processing messages from those partitions.

**Offset Management**: Kafka stores messages durably in brokers, and messages are identified by **offsets**. Consumers pick up where they left off using these offsets, preventing any message loss. If a consumer crashes before committing the latest offset, it may reprocess some messages, leading to **duplicate processing**, but **not data loss**.

### 6. What will happen if an entire consumer group is down? Will data loss occur? Why?
If an **entire consumer group is down**, **data loss will not occur**, as Kafka is designed to store messages durably on brokers, independent of consumer availability. It stores messages in partitions on brokers until they are either consumed and acknowledged or expire based on a **retention policy**. Messages remain available even if no consumers are active. When the consumer group comes back online, it resumes consumption from the last committed offsets.

### 7. Explain consumer lag and how to resolve it?
**Consumer lag** is the difference between the latest offset in a Kafka partition committed by the producer and the last offset processed by a consumer group. It measures how far behind the consumer is from the producer. If a consumer processes messages slower than the producer produces them, the lag increases. This can lead to delayed message processing and potential issues if the lag becomes too large.

**Consumer lag** occurs when consumers fall behind producers. The main strategies to resolve lag are improving consumer throughput, scaling out, and optimizing partition assignment. Monitoring lag and acting proactively can prevent major issues.

### 8. Explain how Kafka tracks message delivery?
- Kafka tracks message delivery using **offsets** and stores them durably.
- Producers use acknowledgments **ACKs** to confirm message delivery to brokers.
- Consumers commit offsets to indicate successfully processed messages.
- Kafka offers **at-most-once, at-least-once, and exactly-once** guarantees based on configuration.

### 9. Compare Kafka vs RabbitMQ, compare messageing frameworks vs MySql (Why Kafka)?
| Feature           | Kafka                                     | RabbitMQ                           | MySQL                         |
| ----------------- | ----------------------------------------- | ---------------------------------- | ----------------------------- |
| Purpose           | Stream processing, high-throughput events | Traditional message queuing, RPC   | Data storage, CRUD operations |
| Throughput        | Millions/sec                              | Thousands/sec                      | Hundreds-Thousands/sec        |
| Message Retention | Configurable (can be permanent)           | Deleted after consumption          | Permanent storage             |
| Use Cases         | Log aggregation, metrics, streaming       | Service communication, task queues | Relational data, transactions |
| Scalability       | Highly scalable (horizontal)              | Moderately scalable                | Vertical scaling primary      |
| Message Ordering  | Guaranteed within partition               | Guaranteed within queue            | N/A                           |
| Best For          | Real-time analytics, event sourcing       | Complex routing, RPC patterns      | Traditional CRUD applications |

### 10. On top of https://github.com/CTYue/Spring-Producer-Consumer
- Write your consumer application with Spring Kafka dependency, set up 3 consumers in a single consumer group.
    Prove message consumption with screenshots.
- Increase number of consumers in a single consumer group, observe what happens, explain your observation.
- Create multiple consumer groups using Spring Kafka, set up different numbers of consumers within each group, observe consumer offset,
- Prove that each consumer group is consuming messages on topics as expected, take screenshots of offset
- Demo different message delivery guarantees in Kafka, with necessary code or configuration changes.
- Note: There's already a docker compose file in above git repo, you can start the 3 brokers (with zookeep) using docker-compose up command.