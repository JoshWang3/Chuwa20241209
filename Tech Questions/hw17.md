# hw17

---

### Core Kafka Concepts

- **Topic**  
  A category/feed to which messages are published. Topics are split into partitions for scalability.

- **Partition**  
  A subset of a topic's data. Each partition is an ordered, immutable sequence of messages. Partitions enable parallel processing.

- **Broker**  
  A Kafka server that stores partitions and serves producers/consumers. A cluster consists of multiple brokers.

- **Consumer Group**  
  A group of consumers that jointly consume a topic. Each partition is consumed by only one consumer in the group (load balancing).

- **Producer**  
  Publishes messages to topics. Producers can send messages to specific partitions (e.g., based on a key).

- **Offset**  
  A unique ID for a message within a partition. Consumers track their position (offset) to resume consumption.

- **Zookeeper**  
  Manages cluster metadata (brokers, topics, partitions) and coordinates brokers (leader election, group membership).

---

### Coordination

1. **Producers → Brokers**  
   Producers publish messages to topic partitions on brokers.  
   → *Example*: A producer sends messages to `TopicA-Partition0` on `Broker1`.

2. **Brokers ↔ Zookeeper**  
   Brokers register with Zookeeper. Zookeeper tracks live brokers and partition leaders.  
   → *Example*: Zookeeper elects `Broker2` as the leader for `TopicB-Partition1`.

3. **Consumer Groups ↔ Brokers**  
   Consumers in a group coordinate via brokers to assign partitions. Offsets are stored in a Kafka topic (`__consumer_offsets`).  
   → *Example*: `ConsumerGroupX` has two consumers, each reading from different partitions of `TopicC`.

4. **Offset Management**  
   Consumers commit offsets to track progress. Brokers manage offsets for consumer groups.  
   → *Example*: After processing offset `150` in `Partition2`, a consumer commits this offset.

---

### Flow Summary
- **Producers** write to **partitions** on **brokers**.
- **Consumer groups** read from partitions, with each consumer handling a subset.
- **Zookeeper** (or KRaft in newer Kafka) ensures cluster coordination.
- **Offsets** track consumer progress within partitions.


## 1. Given N (number of partitions) and M (number of consumers, ) what will happen when N>=M and N<M respectively?

### Partition-to-Consumer Assignment in Kafka

**When `N >= M` (partitions ≥ consumers):**
- Each consumer can be assigned **1 or more partitions**.
- Maximum parallelism: All consumers are active.
- Example:
    - `N = 5`, `M = 3` → Consumers get `2, 2, 1` partitions.

**When `N < M` (partitions < consumers):**
- Only **`N` consumers** are assigned **1 partition each**.
- **`M - N` consumers remain idle** (no partitions assigned).
- Example:
    - `N = 3`, `M = 5` → 3 consumers get 1 partition each; 2 are idle.

---

### Key Rule
A partition can **only be consumed by one consumer per group**, but a consumer can handle multiple partitions.
- Optimal setup: **`N = M`** (1:1 ratio for max balanced parallelism).
- Idle consumers waste resources if `M > N`.


## 2. Explain how brokers work with topics?

### Brokers and Topics in Kafka

1. **Storage**  
   Brokers store **partitions** of topics. Each partition is hosted on a broker as its **leader**, handling read/write requests.

2. **Replication**  
   Partitions are replicated across brokers (followers) for fault tolerance.  
   → *Example*: `TopicX-Partition0` has a leader on `Broker1` and replicas on `Broker2` and `Broker3`.

3. **Load Distribution**  
   Brokers distribute partitions across the cluster to balance load.  
   → *Example*: A topic with 3 partitions may spread them across 3 brokers.

4. **Producer Interaction**  
   Producers send messages to the leader broker of a topic partition.  
   → *Example*: A message for `TopicA-Partition2` is routed to `Broker3` (leader).

5. **Consumer Interaction**  
   Consumers read from the leader broker of assigned partitions.  
   → *Example*: A consumer in `GroupY` fetches data from `Broker1` for `TopicB-Partition1`.

6. **Scalability**  
   Adding brokers allows redistributing partitions (via scaling) to handle higher throughput.

---

### Key Workflow
- Brokers manage **partition lifecycle** (storage, replication, leader election).
- Topics rely on brokers for **durability** (persisted messages) and **availability** (replicas).
- Brokers coordinate with ZooKeeper/KRaft for metadata and leader-follower sync.


## 3. Are messages pushed to consumers or consumers pull messages from topics?

### Push vs. Pull in Kafka

- **Producers → Brokers**  
  **Push model**: Producers actively send (push) messages to brokers/topics.

- **Consumers ← Brokers**  
  **Pull model**: Consumers repeatedly poll (pull) messages from brokers at their own pace.

---

### Why Pull Model for Consumers?
- **Backpressure control**: Consumers avoid overload by fetching only when ready.
- **Flexibility**: Consumers can process messages in batches or replay past data (via offsets).
- **Scalability**: No broker-side tracking of consumer state (e.g., slow consumers).

Example: A consumer polls every 500ms to fetch new messages from `TopicA-Partition0`.


## 4. How to avoid duplicate consumption of messages?
### Avoiding Duplicate Message Consumption

1. **At-Least-Once Delivery (Default)**
    - Offsets are committed **after processing**.
    - Risk: If processing succeeds but offset commit fails (e.g., consumer crash), messages are reprocessed.

2. **Exactly-Once Semantics (EOS)**
    - Enable Kafka's **idempotent producers** and **transactional consumers**.
    - Use `isolation.level=read_committed` to read only committed messages.
    - Example:
      ```java
      props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
      props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
      ```

3. **Idempotent Consumer Logic**
    - Design processing logic to handle duplicates safely (e.g., database upserts, deduplication IDs).

4. **Manual Offset Management**
    - Commit offsets **only after processing completes**. Avoid auto-commit.
    - Example:
      ```java
      consumer.poll().forEach(record -> {
        process(record);
        consumer.commitSync(); // Commit after processing
      });
      ```

---

### Key Trade-Offs
- **At-Least-Once**: Simpler but requires idempotent processing.
- **Exactly-Once**: Overhead for critical use cases (e.g., financial transactions).
- **Idempotency**: Always recommended as a safeguard.


## 5. What will happen if some consumers are down in a consumer group? Will data loss occur? Why?

### Consumer Failure in a Consumer Group

- **Partition Rebalancing**  
  Kafka triggers a **rebalance**: surviving consumers take over the failed consumers' partitions.
    - No data loss occurs because messages are stored durably on brokers, not consumers.

- **Offset Management**
    - If a consumer crashes **before committing offsets**, the new consumer starts from the **last committed offset**, causing **duplicate processing** (not data loss).
    - Example: A consumer processes messages 0-100 but crashes before committing. The new consumer reprocesses from offset 0.

- **Broker Responsibility**
    - Brokers retain messages regardless of consumer status (based on retention policies).

---

### Why No Data Loss?
- **Data is stored on brokers**, not consumers.
- Messages are persisted even if all consumers are offline.

### Key Risk: Reprocessing (Not Loss)
- Ensure **idempotent processing** or use **exactly-once semantics** to handle duplicates.


## 6. What will happen if an entire consumer group is down? Will data loss occur? Why?

### Entire Consumer Group Failure

- **No Data Loss**  
  Data remains stored on Kafka brokers, unaffected by consumer group status. Loss **cannot** occur because messages are broker-persisted, not consumer-dependent.

- **Consumption Pause**
    - If the entire group is down, partitions are **unassigned**.
    - Messages accumulate in the topic until the group resumes or retention expires.

- **Retention Policy**
    - Brokers retain messages based on `retention.ms` (time) or `retention.bytes` (size).
    - Example: If retention is 7 days, messages older than 7 days are deleted, even if unprocessed.

---

### Key Points
- **Data loss only happens if messages expire** before the group resumes.
- Kafka’s durability relies on **broker storage**, not consumer availability.

→ **Solution**: Set appropriate retention policies to match recovery time expectations.


## 7. Explain consumer lag and how to resolve it?

### Consumer Lag & Resolution

**Consumer Lag**  
The difference between the latest message (producer end) and the last processed message (consumer end) in a partition.  
→ *Example*: Partition has offset 500; consumer is at offset 450 → lag = 50.

---

### Causes & Solutions

1. **Slow Consumers**
    - **Optimize processing**: Reduce per-message overhead (e.g., batch processing, async I/O).
    - **Scale consumers**: Add more consumers to the group (if partitions > consumers).

2. **Insufficient Parallelism**
    - **Increase partitions** for the topic (allows more consumers to join the group).

3. **Network/Resource Bottlenecks**
    - **Upgrade consumer hardware** (CPU, memory, network).
    - **Tune Kafka configs**:
        - `fetch.min.bytes`, `max.poll.records` (control batch size).
        - `max.partition.fetch.bytes` (increase fetch buffer).

4. **Producer Overload**
    - **Throttle producers** if message rate exceeds consumer capacity.

---

### Monitoring & Tools
- Use **Kafka Consumer Lag Checker**, **Burrow**, or **Confluent Control Center** to track lag.
- Set alerts for lag thresholds (e.g., lag > 1,000 messages).

### Critical Note
- Lag itself doesn’t cause data loss, but **expired messages** (due to retention policies) can lead to gaps.
- Always monitor retention settings (`retention.ms`, `retention.bytes`).

→ **Best Practice**: Auto-scale consumers based on lag (e.g., Kubernetes Horizontal Pod Autoscaler).


## 8. Explain how Kafka tracks message delivery?
### How Kafka Tracks Message Delivery

1. **Producer Acknowledgments**
    - Producers use `acks` config to track delivery:
        - `acks=0`: No guarantee (fire-and-forget).
        - `acks=1`: Leader broker confirms write (default).
        - `acks=all`: All in-sync replicas (ISR) confirm.

2. **Consumer Offsets**
    - Consumers track progress via **offsets** (per-partition message IDs).
    - Offsets are stored in Kafka’s internal topic `__consumer_offsets`.
    - Example: A consumer commits offset `150` → resumes from `151` after restart.

3. **Exactly-Once Semantics (EOS)**
    - **Idempotent Producer**: Prevents duplicates by assigning unique IDs to messages.
    - **Transactions**: Atomic writes across partitions (producer) and offset commits (consumer).

4. **Broker Replication**
    - Leaders and followers (ISR) sync messages. Delivery is confirmed only when replicas acknowledge.

5. **Delivery Guarantees**
    - **At-Least-Once**: Offsets committed *after* processing (possible duplicates).
    - **Exactly-Once**: Use EOS (idempotent producer + transactional consumer).

---

### Tools for Monitoring
- `kafka-consumer-groups.sh`: Check consumer lag and offsets.
- Confluent Control Center: Visualize message flow and delivery status.

→ **Key Insight**: Kafka tracks delivery via **offsets** and **replication**, but guarantees depend on producer/consumer configurations.


## 9. Compare Kafka vs RabbitMQ, compare messageing frameworks vs MySql (Why Kafka)?
### Kafka vs RabbitMQ

| **Feature**           | **Kafka**                          | **RabbitMQ**                      |
|-----------------------|------------------------------------|-----------------------------------|
| **Design Purpose**    | High-throughput, distributed log   | Message broker (AMQP)             |
| **Messaging Model**   | Publish-Subscribe (persistent log) | Queue-based (point-to-point)      |
| **Throughput**        | Very high (millions/sec)           | Moderate (thousands/sec)          |
| **Latency**           | Higher (batch-focused)             | Lower (real-time)                 |
| **Message Retention** | Long-term (configurable)           | Short-term (deleted after ack)    |
| **Scalability**       | Horizontal scaling (partitions)    | Vertical scaling (clustering)     |
| **Use Cases**         | Log aggregation, event streaming   | Task queues, RPC, enterprise apps |

---

### Messaging Frameworks (Kafka/RabbitMQ) vs MySQL

| **Aspect**       | **Messaging Frameworks**         | **MySQL (DB)**                  |
|------------------|----------------------------------|---------------------------------|
| **Primary Role** | Real-time data streaming/queuing | Persistent data storage & query |
| **Data Model**   | Ephemeral or log-based messages  | Structured tables (ACID)        |
| **Throughput**   | Optimized for high write/read    | Optimized for CRUD transactions |
| **Concurrency**  | Parallel consumers (scalable)    | Limited by locks/transactions   |
| **Use Cases**    | Event-driven systems, decoupling | Transactional data, reporting   |

---

### Why Choose Kafka?
- **High Throughput**: Handles massive data streams (e.g., IoT, logs).
- **Durability**: Messages stored long-term for replayability.
- **Scalability**: Add brokers/partitions to handle load.
- **Stream Processing**: Integrates with tools like Kafka Streams, Flink.
- **Decoupling**: Producers/consumers operate independently.

→ **Example**: Use Kafka for real-time analytics pipelines, not MySQL (slow for streams) or RabbitMQ (lower throughput).  



























