### HW17 Kafka

#### Explain following concepts, and how they coordinate with each other:
Topic
Partition
Broker
Consumer group
Producer
Offset
Zookeeper

```
A topic is a logical category or feed name to which messages are published. It acts as a channel for organizing data.
Topics are the primary way to categorize and organize messages in Kafka.

A partition is a subset of a topic. Each topic is split into one or more partitions, which are the basic units of parallelism in Kafka.
Partitions allow Kafka to scale horizontally by distributing data across multiple brokers.
Messages within a partition are stored in an ordered, immutable sequence.

A broker is a Kafka server that stores data and serves client requests. A Kafka cluster consists of one or more brokers.

A consumer group is a set of consumers that work together to consume messages from one or more topics.
 
A producer is a client application that writes (publishes) messages to Kafka topics.

An offset is a unique sequential ID assigned to each message within a partition. 
It represents the position of a message in the partition.

They coordinate with each other like this:
	Producers write messages to topics, which are divided into partitions for scalability.
	Brokers store these partitions and serve them to consumers.
	Consumer groups read from topics, with each consumer in the group assigned specific partitions.
	Offsets are used to track the reading progress of consumers within partitions.
	Zookeeper oversees the entire system, managing broker metadata and coordinating the cluster.

```

#### Answer following questions:

#### 1. Given N (number of partitions) and M (number of consumers,) what will happen when N>=M and N<M respectively?

```
1, When N >= M (number of partitions >= number of consumers):
	Each consumer will be assigned one or more partitions.
	All consumers will be active and processing data.
	The partitions will be distributed among the consumers, allowing for parallel processing.
All consumers are active, and partitions are distributed among them. This is the ideal scenario for maximizing parallelism.

2, When N < M (number of partitions < number of consumers):
	Each partition will be assigned to only one consumer.
	Some consumers will remain idle as there won't be any partitions left for them to process.
	The idle consumers can act as failover consumers, quickly picking up the slack if an active consumer fails.
Only N consumers are active, and the remaining M−N consumers are idle.
To improve throughput, increase the number of partitions or reduce the number of consumers.

Having too many consumers relative to partitions can increase computational overhead for the Kafka cluster	
```

#### 2. Explain how brokers work with topics?
```
Brokers are the core servers in a Kafka cluster that work with topics in several ways:
	Data Storage: 
		Brokers store topic partitions as log files on their local disks. 
		Each topic is divided into partitions, which can be distributed across multiple brokers for scalability and fault tolerance.
	
	Partition Distribution: 
		A single topic's partitions are spread across multiple brokers in the cluster. 
		For example, if a topic has three partitions, they might be distributed across three different brokers.

	Replication: 
		Brokers handle topic replication to ensure data durability. 
		Each partition can have multiple replicas stored on different brokers. 
		One broker acts as the leader for a partition, while others serve as followers.

	Message Management: 
		When producers send messages to a topic, the broker determines which partition to place the message in based on the message key or round-robin selection.

	Scalability:
		Adding more brokers to a cluster allows for horizontal scaling of topics and partitions2. This distribution enables parallel processing and high throughput.
	
	Broker Discovery: 
		Clients only need to connect to one broker (called a bootstrap server) to discover information about all brokers, topics, and partitions in the cluster.
	
	Fault Tolerance: 
		If a broker fails, other brokers can still serve data for the topics, provided the replication factor is greater than one.

This architecture allows Kafka to handle high-throughput data streaming while maintaining fault tolerance and scalability。
```

#### 3. Are messages pushed to consumers or consumers pull messages from topics?

```
In Kafka, consumers pull messages from topics rather than having messages pushed to them. 

Kafka's pull-based design offers several benefits:
	Scalability: 
		Pull-based systems allow for easier scaling of consumers without affecting broker performance.
	Consumer control: 
		Different consumers can consume messages at their own pace, which is beneficial for handling diverse consumer needs and capabilities.
	Consumption models: 
		Pull-based systems support various consumption models, such as real-time processing and batch processing.
	Load management: 
		Consumers can control their load by deciding when and how much data to fetch, preventing overwhelm during high message volumes.
	Fault tolerance: 
		If a consumer is down or slow, it can catch up later without impacting the broker or other consumers.
```
#### 4. How to avoid duplicate consumption of messages?

```
Here are some strategies:

Kafka-native features:
	Use exactly-once semantics (idempotent producers, transactions)
	Manually commit offsets
	For Kafka Streams, use Processor API for fine-grained control

Consumer-side strategies:
	Implement deduplication, such as using Redis
	Design idempotent consumers
	Use sequence numbers for ordering

System design patterns:
	Implement Transactional Outbox pattern
	Handle consumer failures and rebalancing gracefully

Advanced techniques:
	Leverage Kafka Streams/Connect for built-in exactly-once processing
	Use external deduplication systems for complex scenarios

```
#### 5. What will happen if some consumers are down in a consumer group? Will data loss occur? Why?

```
No Data Loss. Kafka ensures that messages are not lost even if consumers go down.
Rebalancing: 
	Kafka reassigns partitions to active consumers during a rebalance.
Offset Management: 
	Consumers resume processing from the last committed offset.
Fault Tolerance: 
	Kafka's replication and durability features prevent data loss.
```
#### 6. What will happen if an entire consumer group is down? Will data loss occur? Why?

```
No data loss.

Message Persistence: 
	Kafka retains messages in topics based on the configured retention period, regardless of consumer activity.
	Messages will continue to be produced and stored in the Kafka topics, accumulating until the consumer group becomes active again.
Offset Management: 
	The last committed offsets for the consumer group will be retained in the internal "__consumer_offsets" topic.
	When the consumer group restarts, it can resume consumption from the last committed offset stored in the "__consumer_offsets" topic.
Rebalancing on Restart: 
	Upon restarting, the consumer group will trigger a rebalance, reassigning partitions to the available consumers.
```
#### 7. Explain consumer lag and how to resolve it?

```
Consumer Lag is the difference between the latest offset and the current offset.
The reasons of High Lag can be slow consumer processing, insufficient resources, partition imbalance, network issues, or broker issues.

How to resolve it:
	Optimize consumer processing.
	Scale consumers and partitions.
	Increase consumer resources.
	Tune Kafka configuration.
	Monitor and alert for high lag.
```

#### 8. Explain how Kafka tracks message delivery?

```
Kafka tracks message delivery using a combination of offsets, consumer groups, and replication. 

Kafka uses offsets to track the position of each message in a partition.
Kafka uses consumer Groups to coordinate message consumption and store committed offsets in __consumer_offsets.
Kafka uses replication to ensure durability and fault tolerance by replicating partitions across brokers.
Delivery Semantics: Kafka supports at-least-once, at-most-once, and exactly-once delivery.
```

#### 9. Compare Kafka vs RabbitMQ, compare messageing frameworks vs MySql (Why Kafka)?

```
Kafka vs RabbitMQ:
Architecture:
	Kafka: Uses a distributed streaming platform with topics and partitions.
	RabbitMQ: Employs a message broker system with exchanges, queues, and bindings.

Performance:
	Kafka: Can handle millions of messages per second with high throughput.
	RabbitMQ: Optimized for lower throughputs, typically thousands of messages per second.

Scalability:
	Kafka: Designed for horizontal scalability across hundreds or thousands of brokers.
	RabbitMQ: Can be scaled horizontally but not to the same extent as Kafka.

Use Cases:
	Kafka: Ideal for big data scenarios, real-time analytics, and stream processing.
	RabbitMQ: Better suited for complex routing scenarios and low-latency message delivery.

Messaging Frameworks vs MySQL
Data Model:
	Kafka/RabbitMQ: Handle unstructured data streams or messages.
	MySQL: Uses a structured relational data model with tables, rows, and columns.

Processing Paradigm:
	Kafka/RabbitMQ: Designed for real-time stream processing and event-driven architectures.
	MySQL: Focused on transactional processing with ACID properties.

Scalability:
	Kafka/RabbitMQ: Offer horizontal scalability for handling large volumes of data.
	MySQL: Primarily scales vertically, with limitations on a single server.

Performance:
	Kafka: Can process millions of messages per second due to optimized disk I/O and batching.
	MySQL: Optimized for complex queries and joins but may struggle with extremely high throughput scenarios

Why Kafka for high-throughput scenarios:
	Batching: Kafka batches record production and persistence.
	Sequential writes: Ensures efficient hard drive utilization.
	Asynchronous replication: Improves overall system performance.
	Distributed architecture: Allows for horizontal scaling to handle massive data volumes.
```

#### 10. On top of https://github.com/CTYue/Spring-Producer-Consumer
Write your consumer application with Spring Kafka dependency, set up 3 consumers in a single
consumer group.
Prove message consumption with screenshots.
Increase number of consumers in a single consumer group, observe what happens, explain your
observation.
Create multiple consumer groups using Spring Kafka, set up different numbers of consumers within
each group, observe consumer offset,
Prove that each consumer group is consuming messages on topics as expected, take screenshots of
offset records,
Demo different message delivery guarantees in Kafka, with necessary code or configuration changes.
Note: There's already a docker compose file in above git repo, you can start the 3 brokers (with
zookeep) using docker-compose up command.

```
HW17/Coding:

Strat the app:
\Chuwa20241209\Coding\hw17\Spring-Producer-Consumer>docker-compose up -d
```